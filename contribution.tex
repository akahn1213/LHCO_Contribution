\documentclass[letterpaper,11pt]{article}
\usepackage{jheppub}
\usepackage{tikz}

\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{shapes.misc, positioning}



\definecolor{iocolor}{RGB}{169,205,236}
\definecolor{hlvcolor}{RGB}{224,235,245}
\definecolor{hscolor}{RGB}{166,166,166}
\definecolor{layercolor}{RGB}{218,218,218}
\definecolor{featcolor}{RGB}{202,203,229}
\definecolor{grucolor}{RGB}{222, 236, 239}

%\definecolor{hlvlinecolor}{RGB}{12, 178, 236}
%\definecolor{featurelinecolor}{RGB}{100, 159, 206}
\definecolor{hlvlinecolor}{RGB}{0, 148, 197}
\definecolor{featurelinecolor}{RGB}{0, 43, 152}
\definecolor{hslinecolor}{RGB}{85, 85, 85}



\begin{document}

%%%%%%%%%%%%%%%%%%%%%%
\section*{Anomalous Jet Identification via Variational Recurrent Neural Network}
Authors: Alan Kahn, Julia Gonski, In\^{e}s Ochoa, Daniel Williams, Gustaaf Brooijmans \\ \textit{Nevis Laboratories, Columbia University, 136 S Broadway, Irvington NY 10533}\\

%\noindent \textit{Please do not write an introduction to anomaly detection - we will have one introduction at the beginning.  Furthermore, please give your method a concise name - it is fine to say \textbf{Concise Name: Longer Name that is More Specific.}  The length limit is five pages of text (not included references), with fewer pages preferred, and at most one additional page of figures if needed.}

\subsection{Method}
\label{sec:method}

Our method employs a Variational Recurrent Neural Network (VRNN) to model jets as a sequence of constituents. A VRNN is a sequence-modeling architecture which replaces the standard encoder-decoder architecture of a Recurrent Neural Network with a Variational Autoencoder (VAE). This allows the VRNN to perform both sequence modeling as well as \textit{Variational Inference}, which has been shown to be a very powerful tool in the task of Anomaly Detection [Ref]. Being a sequence-modeling architecture, the VRNN is capable of accommodating variable-length inputs, such as the constituent 4-vectors of a jet. Our motivation for choosing a recurrent architecture is to suppress the ability of the model to learn correlations with the number of constituents within a jet. In fixed-length architectures, such as VAEs, the loss function is computed between the input layer and the reconstructed layer, where zero-padded inputs directly affect the value of the loss function, leading to correlations that are difficult to remove. Considering constituents as a sequence rather than as a fixed-length input allows the model to learn a representation in a way which circumvents these drawbacks.


Figure \ref{fig:VRNN} shows a diagram of one VRNN cell. The VAE portion of the architecture is displayed on the top row of layers in the diagram, where an input constitent's four-momentum components are input as a vector $x(t)$, which is encoded into a gaussian distribution in the latent space $z$, and then decoded to produce a reconstruction of the same input constituent's components $y(t)$. The variable $t$ refers to the \textit{time-step}, which advances as the sequence is processed, and can be interpreted as the constituent number currently being processed by the model. 

Inputs to the VRNN consist of sequences of jet 4-vector constituent components $p_{T}$, $\eta$, and $\phi$, where we assume massless constituents. Before training, we apply a pre-processing method which boosts each jet to the same reference mass, energy, and orientation in $\eta-\phi$ space, such that all jets are superficially the same with the only differences being their substructure. In addition, our pre-processing method includes a choice of \textit{sequence ordering}, in which the constituent sequence input into the model is sorted by $k_{t}$-distance instead of by constituent $p_{T}$. In more detail, the $n^{th}$ constituent in the list, $c_{n}$, is determined to be the constituent with the highest $k_{t}$-distance relative to the previous constituent, with the first constituent in the list being the highest $p_{T}$ constituent. This ordering is chosen such that non QCD-like substructure, characterized by two or more separate cores of constituent clusters within in the jet, is more easily characterized by the sequence. When compared to $p_{T}$-sorted constituent ordering, our sequence consistently travels between each cluster, making their existence readily apparent and easy to model. As a result, we have observed a significant boost in performance due to this choice.
\begin{align*}
c_{n} &= max(p_{Tn}\Delta R_{n, n-1})
\end{align*}
The loss function used is very similar to that of an ordinary Variational Autoencoder. It consists of a mean-squared-error (MSE) loss between input constituents and generated output constituents as a reconstruction loss, as well as a weighted KL-Divergence from the latent space prior to the learned approximate posterior distribution. Since softer constituents contribute less to the overall classification of jet substructure, each KL-Divergence term, computed constituent-wise, is weighted by the constituent's $p_{T}$-fraction with respect to the jet's total $p_{T}$, averaged over all jets in the dataset to avoid correlations with constituent multiplicity. An additional weight coefficient is enforced as a hyperparameter, and has been optimized to be 0.1 in our studies. 
\begin{align*}
\mathcal{L}(t)=MSE+0.1\overline{p_T}(t)D_{KL}
\end{align*}
The architecture is built with 16 dimensional hidden layers, including the hidden state, with a two-dimensional latent space. All hyperparameters used result from a hyperparameter optimization scan. 

Our model is trained on the leading and sub-leading jets of each event, where our input datasets consist of the entirety Background and Black Box events. After training, we evaluate each jet in the dataset and assign it an \textit{Anomaly Score}, defined as follows, where $D_{KL}$ is the KL-Divergence between the encoded posterior distribution and learned prior distribution:
\begin{align*}
\text{Anomaly Score} &= 1 - e^{-\overline{D_{KL}}}
\end{align*}


Since the LHC Olympics challenge entails searching for a signal on the event-level instead of the jet-level, we determine an overall \textit{Event Score} by choosing the most anomalous score between the leading and sub-leading jets. To ensure consistency between training scenarios, Event Scores are subject to a transformation in which the mean of the resulting distribution is set to a value of 0.5, and Event Scores closer to 1 correspond to more anomalous events. 

%please introduce the motivation for your method (not anomaly detection in general), how it works, and how you have implemented it. Please include details about how you trained your algorithms and how your picked your hyperparameters.


%VRNN Diagram
\begin{figure}
  \begin{center}
  
    %\def\layersep{2.5cm}
   \includegraphics[width=0.8\textwidth]{imgs/VRNN_Diagram.pdf}
  
  \end{center}
  \caption{A Variational Recurrent Neural Network cell. The $x(t)$ and $y(t)$ layers represent respectively the input constituent and reconstructed constituents' four-momentum components $p_{T}$, $\eta$, and $\phi$. The $phi_{x}$ and $phi_{z}$ layers are \textit{feature-extracting layers} which encode a representation of the features in the input layer $x(t)$ and latent space $z$ respectively. $h(t-1)$ represents the current time-step's hidden state, which is updated each iteration via a transition function between $h(t-1)$, $phi_{x}$, and $phi_{z}$ carried out by a Gated Recurrent Unit (GRU). At each timestep, the prior distribution defined by $\mu_{t}$ and $\sigma_{t}$ is determined from the current hidden state}
  \label{fig:VRNN}
\end{figure}

\subsection{Results on LHC Olympics}
\label{sec:results}

%\noindent \textit{We welcome results on any of the black boxes (BBs) as well as the R\&D dataset.  Please try to minimize any discussion of non-LHCO results.  Figures should be referenced like this: Fig.~\ref{fig:fig1}.}

We first present results with the R\&D dataset. In this study we want to directly investigate the performance of the Anomaly Score, and therefore refrain from imposing cuts on additional variables. Using the Event Score as a discriminator, we aim to reconstruc the $Z'$ mass peak within a contaminated dataset consisting of 895113 background events and 4498 signal events corresponding to a contamination level of 0.5\%. Figure \ref{fig:m_JJ} shows the one-dimensional dijet mass distributions before and after a cut on the Event Score at a value of 0.65, which is chosen to retain enough statistics in the background to display its smoothly falling behavior. We also plot the local significance of the signal, which we have computed for each bin. We see that a cut on the Event Score dramatically increases the significance of the excess. Applying the BinomialExpZ function from RooStats to each bin, and using a relative background uncertainty of 15\%, we observe that the peak of the local significance of the signal increases from $0.18\sigma$ to $2.2\sigma$ at a signal contamination of 0.5\% while still retaining the smoothly falling behavior of the background.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=0.49\textwidth]{imgs/2Prong_Contaminated_0p5_JJ_Mass_Multi.png}
		\includegraphics[width=0.49\textwidth]{imgs/2Prong_Contaminated_0p5_JJ_Mass_EventScore0p65_Multi.png}
	\end{center}
	\caption{Dijet mass distributions before (left) and after (right) a cut on the Event Score, at a signal contamination of 0.5\%}
	\label{fig:m_JJ}
\end{figure}

Our analysis on the Black Box datasets is performed by applying a cut on the Event Score at a value of 0.75, which is chosen to optimize the signal-to-background ratio, as well as restricting the pseudorapidity of the leading and sub-leading jets to less than 0.75 to ensure that central, high momentum-transfer events are being considered. Figure {\ref{fig:bb1}} shows the dijet invariant mass of the dijet for both Black Box 1 and Background datasets. The cut on Event Score reveals an enhancement in $m_{JJ}$ around 4000 GeV. This is consistent with the Black Box 1 signal, which is a new Z' boson with a mass of 3800 GeV decaying to two new particles with masses 732 GeV and 378 GeV.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=0.49\textwidth]{imgs/BB1.png}
		\includegraphics[width=0.49\textwidth]{imgs/BB1_Cut.png}
	\end{center}
	\caption{2-Prong dijet mass distributions before (left) and after (right) a cut on the Event Score, at a signal contamination of 0.5\%}
	\label{fig:bb1}
\end{figure}

The same method applied to Black Box 2 shows no significant excesses in the invariant mass distribution. Furthermore, the effect of the Event Score cut on the $m_{JJ}$ shapes is similar between Black Box 2 and Background datasets. Black Box 2 does not contain any beyond the Standard Model events, and therefore these results are consistent with a QCD-only sample. It is important to note that the model was trained independently on each dataset, and the resulting Event Scores are from entirely unique sets of network weights.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=0.49\textwidth]{imgs/BB2.png}
		\includegraphics[width=0.49\textwidth]{imgs/BB2_Cut.png}
	\end{center}
	\caption{2-Prong dijet mass distributions before (left) and after (right) a cut on the Event Score, at a signal contamination of 0.5\%}
	\label{fig:bb2}
\end{figure}

Figure \ref{fig:bb3} shows results for Black Box 3. The signal in Black Box 3 contained two decays modes of a new 4200 GeV particle, one to two quarks and the other to a trijet resonance with a new 2200 GeV particle in the decay chain. Our model is specifically sensitive to boosted final states, and as a result, we are insensitive to the signal present in this Black Box. 

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=0.49\textwidth]{imgs/BB3.png}
		\includegraphics[width=0.49\textwidth]{imgs/BB3_Cut.png}
	\end{center}
	\caption{2-Prong dijet mass distributions before (left) and after (right) a cut on the Event Score, at a signal contamination of 0.5\%}
	\label{fig:bb3}
\end{figure}


\subsection{Lessons Learned}
\label{sec:lessons}

%\noindent \textit{Please say anything that you learned from the experience in general, what you learned specifically from the results, what you improved after you learned about BB1, what you would change in the future, etc.}

This challenge presented a highly useful avenue for development of our model. Results from the R\&D and Black Box dataset analyses indicate that the VRNN is capable of identifying anomalous objects within a contaminated dataset as long as they can be characterized by sequential data. 
Since the VRNN takes constituent information as input and learns jet substructure without explicit reliance on high level variables, it is less correlated with jet mass than standard substructure variables such as N-subjettiness. 
We learned that the pre-processing method is hugely influential on the performance of the model, in particular the choice of $k_{t}$-ordered sequencing. 
We feel that this is a generalizable takeaway from our study which can be applied to the understanding and use of jet substructure in analysis.

Given these lessons, there are a variety of future opportunities with this application of the VRNN architecture to jet-level anomaly detection. While we limited our scope in this study to be entirely unsupervised without any signal or background information, the RNN and VAE elements of the VRNN provide potential for accommodating more supervised training scenarios. Furthermore, a number of advancements to the architecture, such as a dedicated adversarial mass decorrelation network, or an additional input layer representing high-level features, are worthwhile avenues of exploration to enhance performance while minimizing unwanted correlations. 

%\subsection{Code Availability}
%\label{code:code}

%\noindent \textit{Please consider sharing a link to your code!  All of the sample links will be included in the paper so no need to add those.}

%%%%%%%%%%%%%%%%%%%%%%%%
\acknowledgments

This work was supported by the U.S.~Department of Energy, Office of Science under contract DE-AC02-05CH11231. 

\vspace{10mm}

\noindent \textit{For the references, please use names from Ref.~\cite{hepmllivingreview}.  If your paper is not there or is not updated, please submit a MR!}

\bibliographystyle{jhep}
\bibliography{HEPML}
\end{document}
